{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLV\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mahdi-ebrahimi-per/AI/blob/main/8-%20CLV/CLV.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from persiantools.jdatetime import JalaliDate\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import operator\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "from colorama import Fore\n",
    "\n",
    "from six import StringIO #DM\n",
    "from IPython.display import Image, display\n",
    "import pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jalali to Gregorian\n",
    "\n",
    "# index = 0\n",
    "# for date in DF[DATE_COL]:\n",
    "#     sploted_date = date.split(\"/\")\n",
    "#     toGreg = JalaliDate(sploted_date[0], sploted_date[1], sploted_date[2]).to_gregorian()\n",
    "#     index += 1\n",
    "#     DF[DATE_COL][index] = toGreg.strtime(\"%Y/%M/%D\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Transaction CSV\n",
    "    - ÿßÿ∑ŸÑÿßÿπÿßÿ™ ŸÖÿ±ÿ®Ÿàÿ∑ ÿ®Ÿá ÿ™ÿ±ÿß⁄©ŸÜÿ¥ Ÿáÿß€å ŸÖÿ±ÿ®Ÿàÿ∑ ÿ®Ÿá ÿ™ÿ±ŸÖ€åŸÜÿßŸÑ Ÿáÿßÿ≥ÿ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|    Feature    | Unique Value |\n",
      "+---------------+--------------+\n",
      "|  TERMINAL_NO  |      0       |\n",
      "|    TRANDATE   |      0       |\n",
      "|  RECENCYDATE  |      0       |\n",
      "|  TRANNUMDAYS  |      0       |\n",
      "|   FREQUENCY   |      0       |\n",
      "|     AMOUNT    |      0       |\n",
      "| TRAN_PURCHASE |      0       |\n",
      "|  TRAN_CHARGE  |      0       |\n",
      "|  TRAN_BALANCE |      0       |\n",
      "+---------------+--------------+\n",
      "\u001b[32mDataFrame Haven't any NaN Values üëçüèº\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Load Data Frame\n",
    "# Trans_df = pd.read_csv(\"TRANSACTIONS.csv\", encoding=\"utf-8-sig\")\n",
    "Trans_df = pd.read_excel(\"TRANSACTIONS.xlsx\")\n",
    "\n",
    "Trans_df = Trans_df.iloc[:, :9] #ignore last 2 columns\n",
    "# print(Trans_df.head())\n",
    "\n",
    "\n",
    "# Unique Value\n",
    "unique_counts = PrettyTable()\n",
    "unique_counts.field_names = [\"Feature\", \"Unique Value\"]\n",
    "\n",
    "for column in Trans_df.columns:\n",
    "    count = Trans_df[column].nunique()\n",
    "    unique_counts.add_row([column, count])\n",
    "\n",
    "\n",
    "print(unique_counts)    \n",
    "\n",
    "\n",
    "# NaN Values\n",
    "NaN_include_flag = False\n",
    "Features_includes_NaN = 0\n",
    "for f in list(Trans_df.isna().sum()):\n",
    "    if f > 0:\n",
    "        NaN_include_flag = True\n",
    "        Features_includes_NaN += 1\n",
    "\n",
    "if NaN_include_flag:\n",
    "    print(Fore.RED + f\"DataFrame Includes {sum(list(Trans_df.isna().sum()))} NaN Values in {Features_includes_NaN} Features üëéüèº\" + Fore.RESET)\n",
    "\n",
    "else:\n",
    "    print(Fore.GREEN + \"DataFrame Haven't any NaN Values üëçüèº\" + Fore.RESET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| TERMINAL_NO | TRANDATE | RECENCYDATE | TRANNUMDAYS | FREQUENCY | AMOUNT | TC | TRAN_PURCHASE | TRAN_CHARGE | TRAN_BALANCE |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| ÿ¥ŸÖÿßÿ±Ÿá ÿ™ÿ±ŸÖ€åŸÜÿßŸÑ | ÿ™ÿßÿ±€åÿÆ | ÿ¢ÿÆÿ±€åŸÜ ÿÆÿ±€åÿØ | ÿ™ÿπÿØÿßÿØ ÿ±Ÿàÿ≤ Ÿáÿß€å€å ⁄©Ÿá ÿØÿ± ŸÖÿßŸá ÿ™ÿ±ÿß⁄©ŸÜÿ¥ ÿØÿßÿ¥ÿ™Ÿá | ÿ™ÿπÿØÿßÿØ ÿ™ÿ±ÿß⁄©ŸÜÿ¥ | ŸÖÿ®ŸÑÿ∫ ⁄©ŸÑ ÿ™ÿ±ÿß⁄©ŸÜÿ¥ | ⁄©ÿßÿ±ŸÖÿ≤ÿØ | ÿ™ÿ±ÿß⁄©ŸÜÿ¥ ÿÆÿ±€åÿØ | ÿ™ÿ±ÿß⁄©ŸÜÿ¥ ÿ¥ÿßÿ±⁄ò | ÿ™ÿ±ÿß⁄©ŸÜÿ¥ ÿØÿ±€åÿßŸÅÿ™ ŸÖŸàÿ¨ŸàÿØ€å |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Merchant CSV\n",
    "    -  Ÿáÿß ÿßÿ≥ÿ™ PSP ÿß€åŸÜ ÿØ€åÿ™ÿßÿ≥ÿ™ ŸÖÿ±ÿ®Ÿàÿ∑ ÿ®Ÿá ÿßÿ∑ŸÑÿßÿπÿßÿ™  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+\n",
      "|     Feature     | Unique Value |\n",
      "+-----------------+--------------+\n",
      "|   TERMINAL_NO   |      0       |\n",
      "| MAININSTALLDATE |      0       |\n",
      "|  DISABLED_DATE  |      0       |\n",
      "|  CATEGORY_NAME  |      0       |\n",
      "|   CATEGORYCODE  |      0       |\n",
      "|   PROVINCENAME  |      0       |\n",
      "|   PROVINCECODE  |      0       |\n",
      "|     CITYNAME    |      0       |\n",
      "|     CITYCODE    |      0       |\n",
      "|   PROJECTNAME   |      0       |\n",
      "|   PROJECTCODE   |      0       |\n",
      "|    AGENTNAME    |      0       |\n",
      "|    AGENTCODE    |      0       |\n",
      "|  TERMINALSTATUS |      0       |\n",
      "|     POSTYPE     |      0       |\n",
      "|     POSMODEL    |      0       |\n",
      "|     POSBRAND    |      0       |\n",
      "| WORKFLOWCAPTION |      0       |\n",
      "|       TEL1      |      0       |\n",
      "|   FIRSTNAMEFA   |      0       |\n",
      "|    LASTNAMEFA   |      0       |\n",
      "|  OFFICEADDRESS  |      0       |\n",
      "|      MOBILE     |      0       |\n",
      "+-----------------+--------------+\n",
      "\u001b[32mDataFrame Haven't any NaN Values üëçüèº\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Load Data Frame\n",
    "Merch_df = pd.read_excel(\"MERCHANTS.xlsx\")\n",
    "\n",
    "Merch_df = Merch_df.iloc[:, :23] #drop fetch\n",
    "\n",
    "\n",
    "# Unique Value\n",
    "unique_counts = PrettyTable()\n",
    "unique_counts.field_names = [\"Feature\", \"Unique Value\"]\n",
    "\n",
    "for column in Merch_df.columns:\n",
    "    count = Merch_df[column].nunique()\n",
    "    unique_counts.add_row([column, count])\n",
    "\n",
    "\n",
    "print(unique_counts)    \n",
    "\n",
    "\n",
    "# NaN Values\n",
    "NaN_include_flag = False\n",
    "Features_includes_NaN = 0\n",
    "for f in list(Merch_df.isna().sum()):\n",
    "    if f > 0:\n",
    "        NaN_include_flag = True\n",
    "        Features_includes_NaN += 1\n",
    "\n",
    "if NaN_include_flag:\n",
    "    print(Fore.RED + f\"DataFrame Includes {sum(list(Trans_df.isna().sum()))} NaN Values in {Features_includes_NaN} Features üëéüèº\" + Fore.RESET)\n",
    "\n",
    "else:\n",
    "    print(Fore.GREEN + \"DataFrame Haven't any NaN Values üëçüèº\" + Fore.RESET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join two DataFrame according to \"Merchant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TERMINAL_NO</th>\n",
       "      <th>MAININSTALLDATE</th>\n",
       "      <th>DISABLED_DATE</th>\n",
       "      <th>CATEGORY_NAME</th>\n",
       "      <th>CATEGORYCODE</th>\n",
       "      <th>PROVINCENAME</th>\n",
       "      <th>PROVINCECODE</th>\n",
       "      <th>CITYNAME</th>\n",
       "      <th>CITYCODE</th>\n",
       "      <th>PROJECTNAME</th>\n",
       "      <th>...</th>\n",
       "      <th>MOBILE</th>\n",
       "      <th>TERMINAL_NO</th>\n",
       "      <th>TRANDATE</th>\n",
       "      <th>RECENCYDATE</th>\n",
       "      <th>TRANNUMDAYS</th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>TRAN_PURCHASE</th>\n",
       "      <th>TRAN_CHARGE</th>\n",
       "      <th>TRAN_BALANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TERMINAL_NO, MAININSTALLDATE, DISABLED_DATE, CATEGORY_NAME, CATEGORYCODE, PROVINCENAME, PROVINCECODE, CITYNAME, CITYCODE, PROJECTNAME, PROJECTCODE, AGENTNAME, AGENTCODE, TERMINALSTATUS, POSTYPE, POSMODEL, POSBRAND, WORKFLOWCAPTION, TEL1, FIRSTNAMEFA, LASTNAMEFA, OFFICEADDRESS, MOBILE, TERMINAL_NO, TRANDATE, RECENCYDATE, TRANNUMDAYS, FREQUENCY, AMOUNT, TRAN_PURCHASE, TRAN_CHARGE, TRAN_BALANCE]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DataFrame = pd.concat([Merch_df, Trans_df], axis=1).reindex(Merch_df.index)\n",
    "\n",
    "\n",
    "# print(len(DataFrame.columns))\n",
    "DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop NaNs and not expected type of data in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0, 0), (1, 1), (2, 2), (5, 5)], 6, False)\n",
      "([(6, 0), (7, 1), (8, 2), (11, 5)], 4, True)\n",
      "([('A', 0), ('B', 1), ('C', 2), ('F', 5)], 4, True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1  2\n",
       "0  0   6  A\n",
       "1  1   7  B\n",
       "2  2   8  C\n",
       "5  5  11  F"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Data = list(zip([0, 1, 2, \" \", \"€å\", 5], [6, 7, 8, 9, 10, 11], [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]))\n",
    "df = pd.DataFrame(Data)\n",
    "# print(df) \n",
    "\n",
    "\n",
    "# Drop NaN\n",
    "DataFrame = DataFrame.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def returnExactType(stringOfType): # <class 'int'>  ->  int\n",
    "    start = stringOfType.find(\"\\'\")+1\n",
    "    stop = stringOfType.find(\">\")-1\n",
    "    \n",
    "    return stringOfType[start:stop]\n",
    "\n",
    "\n",
    "def getMaximumTypeOfColumn(DataFrame, Column):\n",
    "    types = set()\n",
    "    \n",
    "    for item in DataFrame[Column]:\n",
    "        types.add(returnExactType(str(type(item))))\n",
    "\n",
    "    typesCounter = dict.fromkeys(list(types), 0)\n",
    "    \n",
    "    for item in DataFrame[Column]:\n",
    "        try : \n",
    "            typesCounter[returnExactType(str(type(item)))] += 1\n",
    "            \n",
    "        except:\n",
    "            pass \n",
    "    \n",
    "    res = max(typesCounter.items(), key=operator.itemgetter(1))[0] \n",
    "    \n",
    "    \n",
    "    if res == \"int\":\n",
    "        return int\n",
    "    \n",
    "    elif res == \"str\":\n",
    "        return str\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"type didn't defind üëéüèº\")\n",
    "    \n",
    "\n",
    "\n",
    "def CheckItemsOfColumnByType(Type, DataFrame, Column) -> bool: # CheckItemsOfColumnByType\n",
    "    return list(map(lambda x:type(x)==Type, list(DataFrame[Column])))\n",
    "    \n",
    "\n",
    "def Drop_FalseFlages(DataFrame, Column, Flages):\n",
    "    \n",
    "    df = DataFrame.copy()\n",
    "    \n",
    "    zipped = dict(zip( list(zip(df[Column], df.index)) , Flages))\n",
    "        \n",
    "    \n",
    "    result = {key:val for key, val in zipped.items() if val == True}\n",
    "\n",
    "    return list(result.keys()), len(DataFrame), len(result.keys())==len(DataFrame[Column])\n",
    "    \n",
    "\n",
    "# (dict_keys([(1, 0), (2, 2), (5, 5)]), 6)\n",
    "\n",
    "\n",
    "def RemoveRowsOfFalseFlag(DataFrame, Column, Cleaned):\n",
    "    \n",
    "    if not Cleaned[2]:\n",
    "        rangeOfFullIndex = set(range(Cleaned[1]))\n",
    "        CleanedIndexed = set(map(lambda x:x[1], Cleaned[0]))\n",
    "        \n",
    "        notExpectedValues_Indexs =  rangeOfFullIndex.difference(CleanedIndexed)\n",
    "        \n",
    "        # print(notExpectedValues_Indexs)\n",
    "        \n",
    "        DataFrame = DataFrame.drop(notExpectedValues_Indexs, axis=0, inplace=True)\n",
    "        \n",
    "        # print(\"df : \", DataFrame)\n",
    "        \n",
    "        return DataFrame\n",
    "    \n",
    "    return DataFrame\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "ColumnTypes = []\n",
    "\n",
    "for col in df:\n",
    "    ColumnTypes.append(getMaximumTypeOfColumn(df, col))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "index = 0\n",
    "for col in df.columns:\n",
    "    Flages = CheckItemsOfColumnByType(ColumnTypes[index], df, col)\n",
    "    Cleaned = Drop_FalseFlages(df, col, Flages)\n",
    "    print(Cleaned)\n",
    "    DropedWrongs = RemoveRowsOfFalseFlag(df, col, Cleaned)\n",
    "    index += 1\n",
    "    \n",
    "    \n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seprate Date into two category \n",
    "    - two category with same len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def seprate_time(DataFrame,TimeStepMonth_Seperator=3 ,time_ColumnName=\"TRANDATE\" ,index_ColumnName=\"TERMINAL_NO\"):\n",
    "    \n",
    "    Time_df = DataFrame[time_ColumnName]\n",
    "    \n",
    "    \n",
    "    # convert Jalalian to Gregorian\n",
    "    df_jalaliToGregorian = []\n",
    "    df_TerminalNumber = []\n",
    "    \n",
    "    # print(DataFrame)\n",
    "    \n",
    "    for index, row in DataFrame.iterrows():\n",
    "        # print(int(row[\"TRANDATE\"]))\n",
    "        gregorian_date = JalaliDate(int(str(int(row[\"TRANDATE\"]))[:4]), int(str(int(row[\"TRANDATE\"]))[4:]),1).to_gregorian()\n",
    "        df_TerminalNumber.append(int(row[\"TERMINAL_NO\"]))\n",
    "        df_jalaliToGregorian.append(gregorian_date)\n",
    "\n",
    "\n",
    "    # Delete ME\n",
    "    df_jalaliToGregorian.append(datetime.date(2000,10,1))\n",
    "    df_jalaliToGregorian.append(datetime.date(2030,10,30))\n",
    "\n",
    "    # Find Min and Max of Dates    \n",
    "    date_min_max = pd.DataFrame(df_jalaliToGregorian).agg([\"min\", \"max\"])\n",
    "\n",
    "    \n",
    "    def time_plus_month(GivenDate, Months=TimeStepMonth_Seperator):\n",
    "        date_format = '%Y/%m/%d'\n",
    "        dtObj = datetime.datetime.strptime(GivenDate, date_format)\n",
    "        future_date = dtObj + relativedelta(months=Months)\n",
    "        return future_date\n",
    "\n",
    "    middle = time_plus_month(str(date_min_max[0]['min']).replace(\"-\",\"/\"))\n",
    "    \n",
    "    Time_Category_Upper = []\n",
    "    Time_Category_Under = [] \n",
    "    \n",
    "    \n",
    "    for date in df_jalaliToGregorian :\n",
    "        if date > middle.date():\n",
    "            Time_Category_Upper.append(date)\n",
    "        else:\n",
    "            Time_Category_Under.append(date)\n",
    "    \n",
    "    \n",
    "    return Time_Category_Upper, Time_Category_Under\n",
    "    \n",
    "    # print(Time_Category_Upper) \n",
    "    # print(Time_Category_Under)\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(seprate_time(Trans_df, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Unique Terminal number per sumOfFeatures For First Category of dates (1 to 3 month)\n",
    "    - use maximum for RECENCYDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TERMINAL_NO</th>\n",
       "      <th>TRANDATE</th>\n",
       "      <th>RECENCYDATE</th>\n",
       "      <th>TRANNUMDAYS</th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>TRAN_PURCHASE</th>\n",
       "      <th>TRAN_CHARGE</th>\n",
       "      <th>TRAN_BALANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TERMINAL_NO, TRANDATE, RECENCYDATE, TRANNUMDAYS, FREQUENCY, AMOUNT, TRAN_PURCHASE, TRAN_CHARGE, TRAN_BALANCE]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trans_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TERMINAL_NO</th>\n",
       "      <th>RECENCYDATE</th>\n",
       "      <th>TRANNUMDAYS</th>\n",
       "      <th>FREQUENCY</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>TRAN_PURCHASE</th>\n",
       "      <th>TRAN_CHARGE</th>\n",
       "      <th>TRAN_BALANCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TERMINAL_NO,  RECENCYDATE, TRANNUMDAYS, FREQUENCY, AMOUNT, TRAN_PURCHASE, TRAN_CHARGE, TRAN_BALANCE]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TerminalNo_Unique = list(Trans_df.TERMINAL_NO.unique())\n",
    "\n",
    "\n",
    "# Columns = [\"TERMINAL_NO\",\" RECENCYDATE\", \"TRANNUMDAYS\", \"FREQUENCY\", \"AMOUNT\", \"TC\", \"TRAN_PURCHASE\", \"TRAN_CHARGE\", 'TRAN_BALANCE']\n",
    "Columns = [\"TERMINAL_NO\",\" RECENCYDATE\", \"TRANNUMDAYS\", \"FREQUENCY\", \"AMOUNT\", \"TRAN_PURCHASE\", \"TRAN_CHARGE\", 'TRAN_BALANCE']\n",
    "\n",
    "\n",
    "Max_RECENCYDATE = []\n",
    "Sum_TRANNUMDAYS = []\n",
    "Sum_FREQUENCY = []\n",
    "Sum_AMOUNT = []\n",
    "# Sum_TC = []\n",
    "Sum_TRAN_PURCHASE = []\n",
    "Sum_TRAN_CHARGE = []\n",
    "Sum_TRAN_BALANCE = []\n",
    "\n",
    "# Max_RECENCYDATE\n",
    "for Terminal in TerminalNo_Unique:\n",
    "    Listed = Trans_df[\"RECENCYDATE\"][(Trans_df[\"TERMINAL_NO\"]==Terminal)]\n",
    "    Max_RECENCYDATE.append(max(Listed))\n",
    "\n",
    "# print(Max_RECENCYDATE)\n",
    "\n",
    "# Sum_TRANNUMDAYS\n",
    "for Terminal in TerminalNo_Unique:\n",
    "    Listed = Trans_df[\"TRANNUMDAYS\"][(Trans_df[\"TERMINAL_NO\"]==Terminal)]\n",
    "    Sum_TRANNUMDAYS.append(sum(Listed))\n",
    "\n",
    "\n",
    "# Sum_FREQUENCY\n",
    "for Terminal in TerminalNo_Unique:\n",
    "    Listed = Trans_df[\"FREQUENCY\"][(Trans_df[\"TERMINAL_NO\"]==Terminal)]\n",
    "    Sum_FREQUENCY.append(sum(Listed))\n",
    "\n",
    "\n",
    "\n",
    "# Sum_AMOUNT\n",
    "for Terminal in TerminalNo_Unique:\n",
    "    Listed = Trans_df[\"AMOUNT\"][(Trans_df[\"TERMINAL_NO\"]==Terminal)]\n",
    "    Sum_AMOUNT.append(sum(Listed))\n",
    "\n",
    "\n",
    "\n",
    "# # Sum_TC\n",
    "# for Terminal in TerminalNo_Unique:\n",
    "#     Listed = Trans_df[\"TC\"][(Trans_df[\"TERMINAL_NO\"]==Terminal)]\n",
    "#     Sum_TC.append(sum(Listed))\n",
    "\n",
    "\n",
    "\n",
    "# Sum_TRAN_PURCHASE\n",
    "for Terminal in TerminalNo_Unique:\n",
    "    Listed = Trans_df[\"TRAN_PURCHASE\"][(Trans_df[\"TERMINAL_NO\"]==Terminal)]\n",
    "    Sum_TRAN_PURCHASE.append(sum(Listed))\n",
    "\n",
    "\n",
    "\n",
    "# Sum_TRAN_CHARGE\n",
    "for Terminal in TerminalNo_Unique:\n",
    "    Listed = Trans_df[\"TRAN_CHARGE\"][(Trans_df[\"TERMINAL_NO\"]==Terminal)]\n",
    "    Sum_TRAN_CHARGE.append(sum(Listed))\n",
    "\n",
    "\n",
    "# Sum_TRAN_BALANCE\n",
    "for Terminal in TerminalNo_Unique:\n",
    "    Listed = Trans_df[\"TRAN_BALANCE\"][(Trans_df[\"TERMINAL_NO\"]==Terminal)]\n",
    "    Sum_TRAN_BALANCE.append(sum(Listed))\n",
    "\n",
    "\n",
    "# zipped = list(zip(TerminalNo_Unique, Max_RECENCYDATE, Sum_TRANNUMDAYS, Sum_FREQUENCY, Sum_AMOUNT, Sum_TC,\n",
    "#                   Sum_TRAN_PURCHASE, Sum_TRAN_CHARGE, Sum_TRAN_BALANCE))\n",
    "\n",
    "zipped = list(zip(TerminalNo_Unique, Max_RECENCYDATE, Sum_TRANNUMDAYS, Sum_FREQUENCY, Sum_AMOUNT,\n",
    "                  Sum_TRAN_PURCHASE, Sum_TRAN_CHARGE, Sum_TRAN_BALANCE))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DataFrame = pd.DataFrame(zipped, columns=Columns)\n",
    "\n",
    "DataFrame\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Terminals Based on (AMOUNT or TC) For Secend Category of dates (4 to 6 month)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required by the scale function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8fa4e06fd3cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSum_AMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(model.labels_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mscale\u001b[1;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"  # noqa\n\u001b[1;32m--> 161\u001b[1;33m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[0m\u001b[0;32m    162\u001b[0m                     \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'the scale function'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                     force_all_finite='allow-nan')\n",
      "\u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[0;32m    670\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required by the scale function."
     ]
    }
   ],
   "source": [
    "TerminalNo_Unique = list(Trans_df.TERMINAL_NO.unique())\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "model = model.fit(scale(Sum_AMOUNT).reshape(-1,1))\n",
    "# print(model.labels_)\n",
    "idx = np.argsort(model.cluster_centers_.sum(axis=1))\n",
    "lut = np.zeros_like(idx)\n",
    "lut[idx] = np.arange(3)\n",
    "\n",
    "\n",
    "Labels = {\"CLUSTER\" : np.array(lut[model.labels_])}\n",
    "Labels_DataFrame = pd.DataFrame(Labels)\n",
    "# print(Labels)\n",
    "\n",
    "\n",
    "\n",
    "uniqueTerminal_Features_Merchant = pd.merge(DataFrame, Merch_df, on=\"TERMINAL_NO\")\n",
    "\n",
    "\n",
    "uniqueTerminal_Features_Merchant_Cluster = pd.concat([uniqueTerminal_Features_Merchant, pd.DataFrame(Labels)], axis=1)\n",
    "\n",
    "DataFrame = uniqueTerminal_Features_Merchant_Cluster\n",
    "\n",
    "DataFrame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame[\"POSBRAND\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "    - Delete Column (Feature) that we don't need it\n",
    "        - It's better to use Code instead name, eg : CityCode instead CityName\n",
    "    - Make Better New Features From Old Features\n",
    "\n",
    "    - is it True :\n",
    "        - Use len Office address as attribute (Office on the right)\n",
    "        - fillna with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MAININSTALLDATE\n",
    "# Use Just year, eg : 1399/03/01 -> 1399 | NaN -> 0\n",
    "DataFrame[\"MAININSTALLDATE\"] = DataFrame[\"MAININSTALLDATE\"].fillna(0)\n",
    "DataFrame[\"MAININSTALLDATE\"] = [int(str(item)[:4]) for item in DataFrame[\"MAININSTALLDATE\"]]\n",
    "\n",
    "\n",
    "# OFFICEADDRESS\n",
    "# Use len insteal full address as a attribute\n",
    "DataFrame[\"MAININSTALLDATE\"] = DataFrame[\"MAININSTALLDATE\"].fillna(0)\n",
    "DataFrame[\"OFFICEADDRESS\"] = [len(str(item)) for item in DataFrame[\"OFFICEADDRESS\"]]\n",
    "\n",
    "\n",
    "# Drop Columns\n",
    "HaveToDrop = [\"DISABLED_DATE\", \"CATEGORY_NAME\", \"PROVINCENAME\",\n",
    "                \"CITYNAME\", \"PROJECTNAME\", \"AGENTNAME\", \"TEL1\",\n",
    "                \"FIRSTNAMEFA\", \"LASTNAMEFA\"]\n",
    "\n",
    "DataFrame = DataFrame.drop(columns=HaveToDrop)\n",
    "\n",
    "\n",
    "# MOBILE\n",
    "# Find Operator and replace Mobile Number With Operator name, and then oneHot it!\n",
    "def MobileOperator():\n",
    "    DataFrame[\"MOBILE\"] = DataFrame[\"MOBILE\"].fillna(0)\n",
    "    DataFrame[\"MOBILE\"] = [int(str(item)[:3]) for item in DataFrame[\"MOBILE\"]]\n",
    "\n",
    "    mci = [912, 990, 919, 910, 911, 918, 917, 916, 915, 914, 913]\n",
    "    irancell = [935, 936, 937, 938, 939, 901, 902, 903, 904, 905]\n",
    "\n",
    "    index = 0\n",
    "    for number in DataFrame[\"MOBILE\"]:\n",
    "        if number in mci:\n",
    "            DataFrame[\"MOBILE\"][index] = \"mci\"\n",
    "            index += 1\n",
    "        \n",
    "        elif number in irancell:\n",
    "            DataFrame[\"MOBILE\"][index] = \"irancell\"\n",
    "            index += 1\n",
    "        \n",
    "        else:\n",
    "            DataFrame[\"MOBILE\"][index] = \"Another\"\n",
    "            index += 1\n",
    "            \n",
    "MobileOperator()\n",
    "    \n",
    "\n",
    "DataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding   \n",
    "    - Use one hot endcoding for string categorized columns\n",
    "        for example : column have two unique value men and women\n",
    "        we use :\n",
    "            10 for men\n",
    "            01 for women\n",
    "            00 for NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def oneHot_Column(DataFrame, ColumnName, inplace=False):\n",
    "    uniqueValues = list(DataFrame[ColumnName].unique())\n",
    "    uniqueValues_withOutNaN = []\n",
    "        \n",
    "    \n",
    "    # uniqueValues without nan\n",
    "    index = 0\n",
    "    for uni in uniqueValues:\n",
    "        try :\n",
    "            math.isnan(uni)\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        uniqueValues_withOutNaN.append(uni)\n",
    "        \n",
    "    \n",
    "        \n",
    "    Transformed = []\n",
    "    \n",
    "    # add full zero if nan value exist\n",
    "    if len(uniqueValues) != len(uniqueValues_withOutNaN):\n",
    "        Transformed.append(\"0\"*len(uniqueValues_withOutNaN))\n",
    "    \n",
    "    \n",
    "    \n",
    "    index = 0\n",
    "    for uniq in uniqueValues_withOutNaN:\n",
    "        BasicZeros = [0] * len(uniqueValues_withOutNaN)\n",
    "        BasicZeros[index] = 1\n",
    "        \n",
    "        Transformed.append(\"\".join(map(str,BasicZeros)))\n",
    "        index += 1\n",
    "        \n",
    "     \n",
    "    unique_oneHot = dict(zip(uniqueValues, Transformed)) # dictionary of {each unique value : OneHot code} \n",
    "\n",
    "\n",
    "    # Replece in column    \n",
    "    if inplace == True:\n",
    "        for i in unique_oneHot.items():\n",
    "            DataFrame[ColumnName].replace(i[0], int(i[1]), inplace=True)\n",
    "            \n",
    "    else:\n",
    "        newDataFrame = DataFrame.copy()\n",
    "        for i in unique_oneHot.items():\n",
    "            newDataFrame[ColumnName] = newDataFrame[ColumnName].replace(i[0], int(i[1])) # Delete me when inplace is True\n",
    "    \n",
    "        return newDataFrame\n",
    "    \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needToOneHot = [\"TERMINALSTATUS\", \"POSTYPE\",\"POSMODEL\", \"POSBRAND\", \"WORKFLOWCAPTION\", \"MOBILE\"]\n",
    "\n",
    "for column in needToOneHot:\n",
    "    oneHot_Column(DataFrame, column, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "DataFrame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally Check DataFrame\n",
    "    - Check DataFrame not include any string and be clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Final Feed DataFrame has string or not\n",
    "\n",
    "flag = False\n",
    "errors = []\n",
    "\n",
    "for column in DataFrame.columns:\n",
    "    if any( [type(item) == str for item in DataFrame[column]]):\n",
    "        flag = True\n",
    "        errors.append(column,\":\" ,DataFrame[column][0],\"|\", DataFrame[column][1])\n",
    "\n",
    "    \n",
    "if flag:\n",
    "    print(Fore.RED, \"Final DataFrame includes String object üëéüèº, Strings Are : \")\n",
    "    for err in errors:\n",
    "        print(err)\n",
    "        \n",
    "    print(Fore.RESET)\n",
    "\n",
    "else:\n",
    "    print(Fore.GREEN, \"Final DataFrame is clean of String object üëçüèº\", Fore.RESET)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-f81a74975765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# With Time Seprator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseprate_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# def Split_TrainTest(DataFrame, Train_ratio=0.7):\n",
    "#     End_Train_index =  round(DataFrame.shape[0]*0.7)\n",
    "#     Train = DataFrame.iloc[:End_Train_index]\n",
    "#     Test =  DataFrame.iloc[End_Train_index:]\n",
    "    \n",
    "#     return Train, Test\n",
    "\n",
    "\n",
    "# Train, Test = Split_TrainTest(DataFrame, 0.7)\n",
    "\n",
    "\n",
    "\n",
    "# With Time Seprator\n",
    "\n",
    "Train, Test = seprate_time(DataFrame)\n",
    "\n",
    "\n",
    "print(f\"{Fore.BLUE}Train : \\n{Fore.RESET}Lenght : {len(Train)}\")\n",
    "print(Train.head())\n",
    "\n",
    "print(f\"\\n{Fore.BLUE}Test : \\n{Fore.RESET}Lenght : {len(Test)}\")\n",
    "print(Test.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Features  = list(DataFrame.columns[ 1: (int(DataFrame.shape[1])-1) ]) # err\n",
    "\n",
    "# Fit on Train\n",
    "y = Train[\"CLUSTER\"]\n",
    "x = Train[Train_Features]\n",
    "\n",
    "clf_Train = tree.DecisionTreeClassifier()\n",
    "clf_Train.fit(x, y)\n",
    "\n",
    "# Predict New Data Using RandomForest\n",
    "clf_Train = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "clf_Train = clf_Train.fit(x, y)\n",
    "\n",
    "# values = [[10, 1, 4, 0, 0, 0]]\n",
    "# prediction = clf_Train.predict(values)  # [1] OR [0]\n",
    "# print( f\"{values[0]}\\n\",\"Hired:\", bool(prediction))\n",
    "\n",
    "\n",
    "\n",
    "# # Fit on Test\n",
    "# y = Test[\"CLUSTER\"]\n",
    "# x = Test[Train_Features]\n",
    "\n",
    "# clf_Test = tree.DecisionTreeClassifier()\n",
    "# clf_Test.fit(x, y)\n",
    "\n",
    "# # Predict New Data Using RandomForest\n",
    "# clf_Test = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# clf_Test = clf_Test.fit(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Test_predicted = pd.DataFrame(dtype=int)\n",
    "\n",
    "for row in Test:\n",
    "    # values = [[10, 1, 4, 0, 0, 0]]\n",
    "    values = [list(row)]\n",
    "    prediction = clf_Train.predict(values)\n",
    "    Test_predicted.append(prediction)\n",
    "\n",
    "\n",
    "Test_StartIndex = round(DataFrame.shape[0]*0.7)\n",
    "\n",
    "\n",
    "# result_df = pd.concat([DataFrame, Test_predicted], axis=1)\n",
    "\n",
    "\n",
    "# print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decesion Tree Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# tree.export_graphviz(clf, out_file=dot_data, feature_names=Train_Features)\n",
    "# graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "# plt = Image(pydot.create_png())\n",
    "# display.svg(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b667cebad148e7b094a58ee81f940c685de1dd70a003a9ccdca4a5792431bee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
